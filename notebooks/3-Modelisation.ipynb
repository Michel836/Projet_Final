{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_Tester un modèle BERT Tiny est une excellente idée pour réduire le temps d'entraînement et utiliser moins de ressources tout en explorant les capacités du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                              title  \\\n",
      "0  0  Palestinians switch off Christmas lights in Be...   \n",
      "1  1  China says Trump call with Taiwan president wo...   \n",
      "2  2   FAIL! The Trump Organization’s Credit Score W...   \n",
      "3  3  Zimbabwe military chief's China trip was norma...   \n",
      "4  4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
      "\n",
      "                                                text  label  text_length  \n",
      "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1         1335  \n",
      "1  BEIJING (Reuters) - U.S. President-elect Donal...      1          373  \n",
      "2  While the controversy over Trump s personal ta...      0         2072  \n",
      "3  BEIJING (Reuters) - A trip to Beijing last wee...      1         2692  \n",
      "4  There has never been a more UNCOURAGEOUS perso...      0         1946  \n",
      "   id                                              title  \\\n",
      "0   0  Live from New York, it's a Trump-Clinton remat...   \n",
      "1   1  Catalan separatists to lose majority in tight ...   \n",
      "2   2  North Carolina governor concedes election to D...   \n",
      "3   3  Draft Senate Iran legislation sets tough new U...   \n",
      "4   4  California governor taps U.S. Representative B...   \n",
      "\n",
      "                                                text  label  \n",
      "0  NEW YORK (Reuters) - Veteran actor and frequen...    1.0  \n",
      "1  BARCELONA (Reuters) - Catalonia s independence...    1.0  \n",
      "2  WINSTON-SALEM, N.C. (Reuters) - North Carolina...    1.0  \n",
      "3  WASHINGTON (Reuters) - Draft legislation respo...    1.0  \n",
      "4  SACRAMENTO, Calif. (Reuters) - California Gove...    1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1218/1218 [00:43<00:00, 27.78it/s, loss=0.00184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1218/1218 [00:43<00:00, 27.73it/s, loss=0.000765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1218/1218 [00:43<00:00, 27.70it/s, loss=0.000262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.99      0.98      0.99      2208\n",
      "    Classe 1       0.99      0.99      0.99      2662\n",
      "\n",
      "    accuracy                           0.99      4870\n",
      "   macro avg       0.99      0.99      0.99      4870\n",
      "weighted avg       0.99      0.99      0.99      4870\n",
      "\n",
      "Test Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.99      0.98      0.99      3750\n",
      "    Classe 1       0.99      0.99      0.99      4363\n",
      "\n",
      "    accuracy                           0.99      8113\n",
      "   macro avg       0.99      0.99      0.99      8113\n",
      "weighted avg       0.99      0.99      0.99      8113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le tokenizer TinyBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "# Préparer les données pour TinyBERT\n",
    "def preprocess_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = torch.tensor(labels.tolist())\n",
    "    return encodings, labels\n",
    "\n",
    "# Charger les datasets\n",
    "train_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/train_cleaned_final.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/test_cleaned.csv\")\n",
    "\n",
    "# Vérifiez les colonnes disponibles\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Séparer les données en train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data['text'], train_data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenisation et encodage des données\n",
    "train_encodings, train_labels = preprocess_data(X_train, y_train, tokenizer)\n",
    "val_encodings, val_labels = preprocess_data(X_val, y_val, tokenizer)\n",
    "test_encodings, test_labels = preprocess_data(test_data['text'], test_data['label'], tokenizer)\n",
    "\n",
    "# Préparer les DataLoaders\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Charger le modèle TinyBERT\n",
    "model = BertForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Entraînement\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Afficher la progression\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de validation\n",
    "print(\"Validation Metrics:\")\n",
    "print(classification_report(val_labels_list, val_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n",
    "\n",
    "# Évaluation sur le jeu de test\n",
    "test_predictions = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de test\n",
    "print(\"Test Metrics:\")\n",
    "print(classification_report(test_labels_list, test_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats pour TinyBERT\n",
    "\n",
    "1. **Validation Metrics** :\n",
    "   - **Classe 0** :\n",
    "     - **Précision** : 99 % (quasi toutes les prédictions pour cette classe sont correctes).\n",
    "     - **Rappel** : 98 % (le modèle a détecté 98 % des exemples réels de cette classe).\n",
    "     - **F1-score** : 99 % (équilibre entre précision et rappel).\n",
    "   - **Classe 1** :\n",
    "     - Tous les scores sont proches de 99 %, ce qui indique une excellente performance.\n",
    "   - **Précision globale** : 99 % sur le jeu de validation (4870 exemples).\n",
    "\n",
    "2. **Test Metrics** :\n",
    "   - **Classe 0 et Classe 1** :\n",
    "     - Tous les scores (précision, rappel, F1-score) sont autour de 99 %.\n",
    "   - **Précision globale** : 99 % sur le jeu de test (8113 exemples).\n",
    "\n",
    "3. **Moyennes pondérées et globales** :\n",
    "   - **Macro avg** et **Weighted avg** à 99 % confirment que les deux classes sont bien équilibrées dans leurs performances.\n",
    "\n",
    "---\n",
    "\n",
    "## Points positifs :\n",
    "- **Précision élevée** : Indique que TinyBERT gère très bien votre jeu de données.\n",
    "- **Équilibre entre les classes** : Les métriques pour les deux classes sont quasiment identiques, ce qui suggère que le modèle n'est pas biaisé vers une classe particulière.\n",
    "- **Généralisation solide** : Les performances sur le jeu de validation et de test sont similaires, ce qui montre une bonne généralisation du modèle.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_Pour tester et évaluer un modèle BERT Small, le processus est similaire à celui de TinyBERT. Hugging Face propose des modèles BERT Small, souvent appelés \"distilled\" ou \"light\". Voici comment procéder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                              title  \\\n",
      "0  0  Palestinians switch off Christmas lights in Be...   \n",
      "1  1  China says Trump call with Taiwan president wo...   \n",
      "2  2   FAIL! The Trump Organization’s Credit Score W...   \n",
      "3  3  Zimbabwe military chief's China trip was norma...   \n",
      "4  4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
      "\n",
      "                                                text  label  text_length  \n",
      "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1         1335  \n",
      "1  BEIJING (Reuters) - U.S. President-elect Donal...      1          373  \n",
      "2  While the controversy over Trump s personal ta...      0         2072  \n",
      "3  BEIJING (Reuters) - A trip to Beijing last wee...      1         2692  \n",
      "4  There has never been a more UNCOURAGEOUS perso...      0         1946  \n",
      "   id                                              title  \\\n",
      "0   0  Live from New York, it's a Trump-Clinton remat...   \n",
      "1   1  Catalan separatists to lose majority in tight ...   \n",
      "2   2  North Carolina governor concedes election to D...   \n",
      "3   3  Draft Senate Iran legislation sets tough new U...   \n",
      "4   4  California governor taps U.S. Representative B...   \n",
      "\n",
      "                                                text  label  \n",
      "0  NEW YORK (Reuters) - Veteran actor and frequen...    1.0  \n",
      "1  BARCELONA (Reuters) - Catalonia s independence...    1.0  \n",
      "2  WINSTON-SALEM, N.C. (Reuters) - North Carolina...    1.0  \n",
      "3  WASHINGTON (Reuters) - Draft legislation respo...    1.0  \n",
      "4  SACRAMENTO, Calif. (Reuters) - California Gove...    1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1218/1218 [01:06<00:00, 18.23it/s, loss=0.000439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1218/1218 [01:07<00:00, 18.16it/s, loss=0.000181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1218/1218 [01:07<00:00, 18.16it/s, loss=0.00063] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.99      0.98      0.99      2208\n",
      "    Classe 1       0.98      0.99      0.99      2662\n",
      "\n",
      "    accuracy                           0.99      4870\n",
      "   macro avg       0.99      0.99      0.99      4870\n",
      "weighted avg       0.99      0.99      0.99      4870\n",
      "\n",
      "Test Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       1.00      0.98      0.99      3750\n",
      "    Classe 1       0.98      1.00      0.99      4363\n",
      "\n",
      "    accuracy                           0.99      8113\n",
      "   macro avg       0.99      0.99      0.99      8113\n",
      "weighted avg       0.99      0.99      0.99      8113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le tokenizer BERT Small\n",
    "model_name = \"prajjwal1/bert-small\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Préparer les données pour BERT Small\n",
    "def preprocess_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = torch.tensor(labels.tolist())\n",
    "    return encodings, labels\n",
    "\n",
    "# Charger les datasets\n",
    "train_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/train_cleaned_final.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/test_cleaned.csv\")\n",
    "\n",
    "# Vérifiez les colonnes disponibles\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Séparer les données en train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data['text'], train_data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenisation et encodage des données\n",
    "train_encodings, train_labels = preprocess_data(X_train, y_train, tokenizer)\n",
    "val_encodings, val_labels = preprocess_data(X_val, y_val, tokenizer)\n",
    "test_encodings, test_labels = preprocess_data(test_data['text'], test_data['label'], tokenizer)\n",
    "\n",
    "# Préparer les DataLoaders\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Charger le modèle BERT Small\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Entraînement\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Afficher la progression\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de validation\n",
    "print(\"Validation Metrics:\")\n",
    "print(classification_report(val_labels_list, val_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n",
    "\n",
    "# Évaluation sur le jeu de test\n",
    "test_predictions = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de test\n",
    "print(\"Test Metrics:\")\n",
    "print(classification_report(test_labels_list, test_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats pour **BERT Small**\n",
    "\n",
    "### **1. Validation Metrics**\n",
    "- **Classe 0 (support : 2208)** :\n",
    "  - **Précision** : 99 % (quasiment toutes les prédictions positives pour cette classe sont correctes).\n",
    "  - **Rappel** : 98 % (le modèle a identifié 98 % des vrais exemples de cette classe).\n",
    "  - **F1-score** : 99 %, ce qui indique un excellent équilibre entre précision et rappel.\n",
    "- **Classe 1 (support : 2662)** :\n",
    "  - **Précision** : 98 % (légèrement inférieure à celle de la Classe 0, mais reste excellente).\n",
    "  - **Rappel** : 99 % (presque tous les exemples réels de cette classe ont été détectés).\n",
    "- **Précision globale** : **99 %** sur le jeu de validation, montrant une très bonne généralisation.\n",
    "\n",
    "### **2. Test Metrics**\n",
    "- **Classe 0 (support : 3750)** :\n",
    "  - **Précision** : 100 % (toutes les prédictions pour cette classe sont exactes).\n",
    "  - **Rappel** : 98 % (quelques exemples de la Classe 0 n’ont pas été détectés).\n",
    "  - **F1-score** : 99 %, identique à la validation.\n",
    "- **Classe 1 (support : 4363)** :\n",
    "  - **Précision** : 98 %, cohérent avec les résultats sur le jeu de validation.\n",
    "  - **Rappel** : 100 %, aucun exemple de la Classe 1 n’a été manqué.\n",
    "- **Précision globale** : **99 %**, montrant une excellente performance sur le jeu de test.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparaison avec TinyBERT\n",
    "\n",
    "| Modèle          | Validation Accuracy | Test Accuracy | Macro Avg Precision | Macro Avg Recall | Macro Avg F1-Score |\n",
    "|------------------|---------------------|---------------|---------------------|------------------|---------------------|\n",
    "| **TinyBERT**    | 99 %               | 99 %          | 99 %               | 99 %            | 99 %               |\n",
    "| **BERT Small**  | 99 %               | 99 %          | 99 %               | 99 %            | 99 %               |\n",
    "\n",
    "Les résultats de BERT Small et TinyBERT sont similaires, ce qui est courant lorsque le dataset est bien équilibré et de taille modérée. Cependant :\n",
    "- BERT Small a une capacité légèrement supérieure, ce qui peut le rendre plus performant sur des tâches ou datasets plus complexes.\n",
    "- TinyBERT est plus rapide et léger, ce qui est avantageux pour les systèmes avec des ressources limitées.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommandations\n",
    "1. **Choix du modèle** :\n",
    "   - Si les performances actuelles suffisent pour votre projet, **TinyBERT** est un excellent choix grâce à son efficacité.\n",
    "   - Si vous prévoyez de traiter des datasets plus complexes, **BERT Small** pourrait offrir un avantage.\n",
    "\n",
    "2. **Prochaines étapes** :\n",
    "   - **Déploiement** : Envisagez de déployer le modèle via une API ou une application web (par exemple, Flask, FastAPI).\n",
    "   - **Évaluation en conditions réelles** : Testez le modèle sur des données réelles pour vérifier sa robustesse.\n",
    "   - **Fine-tuning sur d'autres tâches** : Si vous souhaitez explorer d’autres tâches (comme la classification multi-label ou la détection d’entités nommées), BERT Small pourrait être un bon point de départ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_Lazy Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy Predict est une bibliothèque Python qui permet de tester rapidement plusieurs modèles de machine learning (ML) sur un dataset sans devoir coder chaque pipeline individuellement. Elle est utile pour :\n",
    "\n",
    "Obtenir une vue d'ensemble des performances de différents modèles ML.\n",
    "Comparer les modèles pour un problème spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.13-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (8.1.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (1.4.2)\n",
      "Collecting lightgbm (from lazypredict)\n",
      "  Downloading lightgbm-4.5.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting xgboost (from lazypredict)\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from click->lazypredict) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lightgbm->lazypredict) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lightgbm->lazypredict) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from pandas->lazypredict) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from pandas->lazypredict) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from pandas->lazypredict) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from scikit-learn->lazypredict) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n",
      "Downloading lazypredict-0.2.13-py2.py3-none-any.whl (12 kB)\n",
      "Downloading lightgbm-4.5.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 73.9 MB/s eta 0:00:00\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ----------------- --------------------- 56.1/124.9 MB 275.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- --- 114.3/124.9 MB 270.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- 124.9/124.9 MB 221.8 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost, lightgbm, lazypredict\n",
      "Successfully installed lazypredict-0.2.13 lightgbm-4.5.0 xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install lazypredict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible : True\n",
      "Nom du GPU : NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Vérifier si CUDA est disponible\n",
    "print(\"GPU disponible :\", torch.cuda.is_available())\n",
    "print(\"Nom du GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU détecté\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appareil utilisé : cuda\n",
      "Colonnes dans le fichier d'entraînement : Index(['id', 'title', 'text', 'label', 'text_length'], dtype='object')\n",
      "Colonnes dans le fichier de test : Index(['id', 'title', 'text', 'label'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [05:14<00:07,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10584, number of negative: 8893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 19477, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543410 -> initscore=0.174079\n",
      "[LightGBM] [Info] Start training from score 0.174079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [05:16<00:00,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performances des modèles Lazy Predict :\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "SVC                                0.97               0.97     0.97      0.97   \n",
      "LinearDiscriminantAnalysis         0.96               0.96     0.96      0.96   \n",
      "RidgeClassifierCV                  0.96               0.96     0.96      0.96   \n",
      "RidgeClassifier                    0.96               0.96     0.96      0.96   \n",
      "LinearSVC                          0.96               0.96     0.96      0.96   \n",
      "CalibratedClassifierCV             0.96               0.96     0.96      0.96   \n",
      "LogisticRegression                 0.96               0.96     0.96      0.96   \n",
      "NearestCentroid                    0.96               0.96     0.96      0.96   \n",
      "SGDClassifier                      0.96               0.96     0.96      0.96   \n",
      "LGBMClassifier                     0.95               0.95     0.95      0.95   \n",
      "NuSVC                              0.95               0.95     0.95      0.95   \n",
      "Perceptron                         0.94               0.94     0.94      0.94   \n",
      "PassiveAggressiveClassifier        0.93               0.93     0.93      0.93   \n",
      "BaggingClassifier                  0.93               0.93     0.93      0.93   \n",
      "RandomForestClassifier             0.93               0.93     0.93      0.93   \n",
      "ExtraTreesClassifier               0.93               0.92     0.92      0.93   \n",
      "AdaBoostClassifier                 0.92               0.92     0.92      0.92   \n",
      "QuadraticDiscriminantAnalysis      0.88               0.89     0.89      0.88   \n",
      "DecisionTreeClassifier             0.88               0.87     0.87      0.88   \n",
      "BernoulliNB                        0.86               0.86     0.86      0.86   \n",
      "KNeighborsClassifier               0.81               0.82     0.82      0.81   \n",
      "GaussianNB                         0.77               0.77     0.77      0.77   \n",
      "ExtraTreeClassifier                0.71               0.70     0.70      0.71   \n",
      "LabelSpreading                     0.46               0.51     0.51      0.31   \n",
      "LabelPropagation                   0.46               0.51     0.51      0.31   \n",
      "DummyClassifier                    0.55               0.50     0.50      0.39   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "SVC                                 51.66  \n",
      "LinearDiscriminantAnalysis           0.65  \n",
      "RidgeClassifierCV                    1.44  \n",
      "RidgeClassifier                      0.27  \n",
      "LinearSVC                            0.48  \n",
      "CalibratedClassifierCV               1.63  \n",
      "LogisticRegression                   0.24  \n",
      "NearestCentroid                      0.27  \n",
      "SGDClassifier                        0.74  \n",
      "LGBMClassifier                       2.57  \n",
      "NuSVC                               66.19  \n",
      "Perceptron                           0.22  \n",
      "PassiveAggressiveClassifier          0.26  \n",
      "BaggingClassifier                   70.96  \n",
      "RandomForestClassifier              31.42  \n",
      "ExtraTreesClassifier                 5.03  \n",
      "AdaBoostClassifier                  31.67  \n",
      "QuadraticDiscriminantAnalysis        0.43  \n",
      "DecisionTreeClassifier               9.57  \n",
      "BernoulliNB                          0.26  \n",
      "KNeighborsClassifier                 0.41  \n",
      "GaussianNB                           0.22  \n",
      "ExtraTreeClassifier                  0.25  \n",
      "LabelSpreading                      17.21  \n",
      "LabelPropagation                    14.60  \n",
      "DummyClassifier                      0.16  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Vérifier si un GPU est disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Appareil utilisé :\", device)\n",
    "\n",
    "# Charger les fichiers\n",
    "train_file = \"C:/Users/chume/Projet_Final/data/processed/train_cleaned_final.csv\"\n",
    "test_file = \"C:/Users/chume/Projet_Final/data/processed/test_cleaned.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Vérifiez les colonnes\n",
    "print(\"Colonnes dans le fichier d'entraînement :\", train_data.columns)\n",
    "print(\"Colonnes dans le fichier de test :\", test_data.columns)\n",
    "\n",
    "# Assurez-vous d'utiliser les bonnes colonnes pour les textes et les labels\n",
    "X = train_data['text']\n",
    "y = train_data['label']\n",
    "\n",
    "# Vectorisation des données avec TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Limitez à 10 000 pour la mémoire GPU\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Réduction de dimension avec TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)  # Réduit à 300 dimensions\n",
    "X_reduced = svd.fit_transform(X_vec)\n",
    "\n",
    "# Division des données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Utilisation de Lazy Predict\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Performances des modèles Lazy Predict :\")\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommandations pour le projet de fake news\n",
    "1. SVC (Support Vector Classifier)\n",
    "Performances : Le meilleur modèle en termes d'Accuracy (97 %).\n",
    "Temps : Relativement rapide (51.66 s) mais peut être long pour de plus grands datasets.\n",
    "Adapté pour : Datasets avec une séparation linéaire ou semi-linéaire des classes.\n",
    "2. LinearDiscriminantAnalysis\n",
    "Performances : Similaire à SVC (96 %), mais bien plus rapide (0.65 s).\n",
    "Avantages : Idéal pour des implémentations rapides ou une première évaluation.\n",
    "3. RidgeClassifier et RidgeClassifierCV\n",
    "Performances : Identiques à LinearDiscriminantAnalysis (96 %).\n",
    "Avantages : Plus rapide et facile à intégrer pour des tâches simples.\n",
    "4. LGBMClassifier\n",
    "Performances : Très bon (95 %) avec un temps raisonnable (2.57 s).\n",
    "Avantages : Modèle basé sur le boosting, efficace pour capturer des relations non linéaires."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
