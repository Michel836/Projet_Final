{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_Tester un modèle BERT Tiny est une excellente idée pour réduire le temps d'entraînement et utiliser moins de ressources tout en explorant les capacités du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                              title  \\\n",
      "0  0  Palestinians switch off Christmas lights in Be...   \n",
      "1  1  China says Trump call with Taiwan president wo...   \n",
      "2  2   FAIL! The Trump Organization’s Credit Score W...   \n",
      "3  3  Zimbabwe military chief's China trip was norma...   \n",
      "4  4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
      "\n",
      "                                                text  label  text_length  \n",
      "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1         1335  \n",
      "1  BEIJING (Reuters) - U.S. President-elect Donal...      1          373  \n",
      "2  While the controversy over Trump s personal ta...      0         2072  \n",
      "3  BEIJING (Reuters) - A trip to Beijing last wee...      1         2692  \n",
      "4  There has never been a more UNCOURAGEOUS perso...      0         1946  \n",
      "   id                                              title  \\\n",
      "0   0  Live from New York, it's a Trump-Clinton remat...   \n",
      "1   1  Catalan separatists to lose majority in tight ...   \n",
      "2   2  North Carolina governor concedes election to D...   \n",
      "3   3  Draft Senate Iran legislation sets tough new U...   \n",
      "4   4  California governor taps U.S. Representative B...   \n",
      "\n",
      "                                                text  label  \n",
      "0  NEW YORK (Reuters) - Veteran actor and frequen...    1.0  \n",
      "1  BARCELONA (Reuters) - Catalonia s independence...    1.0  \n",
      "2  WINSTON-SALEM, N.C. (Reuters) - North Carolina...    1.0  \n",
      "3  WASHINGTON (Reuters) - Draft legislation respo...    1.0  \n",
      "4  SACRAMENTO, Calif. (Reuters) - California Gove...    1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1218/1218 [00:43<00:00, 27.78it/s, loss=0.00184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1218/1218 [00:43<00:00, 27.73it/s, loss=0.000765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1218/1218 [00:43<00:00, 27.70it/s, loss=0.000262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.99      0.98      0.99      2208\n",
      "    Classe 1       0.99      0.99      0.99      2662\n",
      "\n",
      "    accuracy                           0.99      4870\n",
      "   macro avg       0.99      0.99      0.99      4870\n",
      "weighted avg       0.99      0.99      0.99      4870\n",
      "\n",
      "Test Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.99      0.98      0.99      3750\n",
      "    Classe 1       0.99      0.99      0.99      4363\n",
      "\n",
      "    accuracy                           0.99      8113\n",
      "   macro avg       0.99      0.99      0.99      8113\n",
      "weighted avg       0.99      0.99      0.99      8113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le tokenizer TinyBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "# Préparer les données pour TinyBERT\n",
    "def preprocess_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = torch.tensor(labels.tolist())\n",
    "    return encodings, labels\n",
    "\n",
    "# Charger les datasets\n",
    "train_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/train_cleaned_final.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/test_cleaned.csv\")\n",
    "\n",
    "# Vérifiez les colonnes disponibles\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Séparer les données en train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data['text'], train_data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenisation et encodage des données\n",
    "train_encodings, train_labels = preprocess_data(X_train, y_train, tokenizer)\n",
    "val_encodings, val_labels = preprocess_data(X_val, y_val, tokenizer)\n",
    "test_encodings, test_labels = preprocess_data(test_data['text'], test_data['label'], tokenizer)\n",
    "\n",
    "# Préparer les DataLoaders\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Charger le modèle TinyBERT\n",
    "model = BertForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Entraînement\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Afficher la progression\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de validation\n",
    "print(\"Validation Metrics:\")\n",
    "print(classification_report(val_labels_list, val_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n",
    "\n",
    "# Évaluation sur le jeu de test\n",
    "test_predictions = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de test\n",
    "print(\"Test Metrics:\")\n",
    "print(classification_report(test_labels_list, test_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats pour TinyBERT\n",
    "\n",
    "1. **Validation Metrics** :\n",
    "   - **Classe 0** :\n",
    "     - **Précision** : 99 % (quasi toutes les prédictions pour cette classe sont correctes).\n",
    "     - **Rappel** : 98 % (le modèle a détecté 98 % des exemples réels de cette classe).\n",
    "     - **F1-score** : 99 % (équilibre entre précision et rappel).\n",
    "   - **Classe 1** :\n",
    "     - Tous les scores sont proches de 99 %, ce qui indique une excellente performance.\n",
    "   - **Précision globale** : 99 % sur le jeu de validation (4870 exemples).\n",
    "\n",
    "2. **Test Metrics** :\n",
    "   - **Classe 0 et Classe 1** :\n",
    "     - Tous les scores (précision, rappel, F1-score) sont autour de 99 %.\n",
    "   - **Précision globale** : 99 % sur le jeu de test (8113 exemples).\n",
    "\n",
    "3. **Moyennes pondérées et globales** :\n",
    "   - **Macro avg** et **Weighted avg** à 99 % confirment que les deux classes sont bien équilibrées dans leurs performances.\n",
    "\n",
    "---\n",
    "\n",
    "## Points positifs :\n",
    "- **Précision élevée** : Indique que TinyBERT gère très bien votre jeu de données.\n",
    "- **Équilibre entre les classes** : Les métriques pour les deux classes sont quasiment identiques, ce qui suggère que le modèle n'est pas biaisé vers une classe particulière.\n",
    "- **Généralisation solide** : Les performances sur le jeu de validation et de test sont similaires, ce qui montre une bonne généralisation du modèle.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_Pour tester et évaluer un modèle BERT Small, le processus est similaire à celui de TinyBERT. Hugging Face propose des modèles BERT Small, souvent appelés \"distilled\" ou \"light\". Voici comment procéder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                              title  \\\n",
      "0  0  Palestinians switch off Christmas lights in Be...   \n",
      "1  1  China says Trump call with Taiwan president wo...   \n",
      "2  2   FAIL! The Trump Organization’s Credit Score W...   \n",
      "3  3  Zimbabwe military chief's China trip was norma...   \n",
      "4  4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
      "\n",
      "                                                text  label  text_length  \n",
      "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1         1335  \n",
      "1  BEIJING (Reuters) - U.S. President-elect Donal...      1          373  \n",
      "2  While the controversy over Trump s personal ta...      0         2072  \n",
      "3  BEIJING (Reuters) - A trip to Beijing last wee...      1         2692  \n",
      "4  There has never been a more UNCOURAGEOUS perso...      0         1946  \n",
      "   id                                              title  \\\n",
      "0   0  Live from New York, it's a Trump-Clinton remat...   \n",
      "1   1  Catalan separatists to lose majority in tight ...   \n",
      "2   2  North Carolina governor concedes election to D...   \n",
      "3   3  Draft Senate Iran legislation sets tough new U...   \n",
      "4   4  California governor taps U.S. Representative B...   \n",
      "\n",
      "                                                text  label  \n",
      "0  NEW YORK (Reuters) - Veteran actor and frequen...    1.0  \n",
      "1  BARCELONA (Reuters) - Catalonia s independence...    1.0  \n",
      "2  WINSTON-SALEM, N.C. (Reuters) - North Carolina...    1.0  \n",
      "3  WASHINGTON (Reuters) - Draft legislation respo...    1.0  \n",
      "4  SACRAMENTO, Calif. (Reuters) - California Gove...    1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1218/1218 [01:06<00:00, 18.23it/s, loss=0.000439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1218/1218 [01:07<00:00, 18.16it/s, loss=0.000181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1218/1218 [01:07<00:00, 18.16it/s, loss=0.00063] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.99      0.98      0.99      2208\n",
      "    Classe 1       0.98      0.99      0.99      2662\n",
      "\n",
      "    accuracy                           0.99      4870\n",
      "   macro avg       0.99      0.99      0.99      4870\n",
      "weighted avg       0.99      0.99      0.99      4870\n",
      "\n",
      "Test Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       1.00      0.98      0.99      3750\n",
      "    Classe 1       0.98      1.00      0.99      4363\n",
      "\n",
      "    accuracy                           0.99      8113\n",
      "   macro avg       0.99      0.99      0.99      8113\n",
      "weighted avg       0.99      0.99      0.99      8113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le tokenizer BERT Small\n",
    "model_name = \"prajjwal1/bert-small\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Préparer les données pour BERT Small\n",
    "def preprocess_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = torch.tensor(labels.tolist())\n",
    "    return encodings, labels\n",
    "\n",
    "# Charger les datasets\n",
    "train_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/train_cleaned_final.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/chume/Projet_Final/data/processed/test_cleaned.csv\")\n",
    "\n",
    "# Vérifiez les colonnes disponibles\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Séparer les données en train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data['text'], train_data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenisation et encodage des données\n",
    "train_encodings, train_labels = preprocess_data(X_train, y_train, tokenizer)\n",
    "val_encodings, val_labels = preprocess_data(X_val, y_val, tokenizer)\n",
    "test_encodings, test_labels = preprocess_data(test_data['text'], test_data['label'], tokenizer)\n",
    "\n",
    "# Préparer les DataLoaders\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Charger le modèle BERT Small\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Entraînement\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Afficher la progression\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de validation\n",
    "print(\"Validation Metrics:\")\n",
    "print(classification_report(val_labels_list, val_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n",
    "\n",
    "# Évaluation sur le jeu de test\n",
    "test_predictions = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapport de classification sur le jeu de test\n",
    "print(\"Test Metrics:\")\n",
    "print(classification_report(test_labels_list, test_predictions, target_names=[\"Classe 0\", \"Classe 1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats pour **BERT Small**\n",
    "\n",
    "### **1. Validation Metrics**\n",
    "- **Classe 0 (support : 2208)** :\n",
    "  - **Précision** : 99 % (quasiment toutes les prédictions positives pour cette classe sont correctes).\n",
    "  - **Rappel** : 98 % (le modèle a identifié 98 % des vrais exemples de cette classe).\n",
    "  - **F1-score** : 99 %, ce qui indique un excellent équilibre entre précision et rappel.\n",
    "- **Classe 1 (support : 2662)** :\n",
    "  - **Précision** : 98 % (légèrement inférieure à celle de la Classe 0, mais reste excellente).\n",
    "  - **Rappel** : 99 % (presque tous les exemples réels de cette classe ont été détectés).\n",
    "- **Précision globale** : **99 %** sur le jeu de validation, montrant une très bonne généralisation.\n",
    "\n",
    "### **2. Test Metrics**\n",
    "- **Classe 0 (support : 3750)** :\n",
    "  - **Précision** : 100 % (toutes les prédictions pour cette classe sont exactes).\n",
    "  - **Rappel** : 98 % (quelques exemples de la Classe 0 n’ont pas été détectés).\n",
    "  - **F1-score** : 99 %, identique à la validation.\n",
    "- **Classe 1 (support : 4363)** :\n",
    "  - **Précision** : 98 %, cohérent avec les résultats sur le jeu de validation.\n",
    "  - **Rappel** : 100 %, aucun exemple de la Classe 1 n’a été manqué.\n",
    "- **Précision globale** : **99 %**, montrant une excellente performance sur le jeu de test.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparaison avec TinyBERT\n",
    "\n",
    "| Modèle          | Validation Accuracy | Test Accuracy | Macro Avg Precision | Macro Avg Recall | Macro Avg F1-Score |\n",
    "|------------------|---------------------|---------------|---------------------|------------------|---------------------|\n",
    "| **TinyBERT**    | 99 %               | 99 %          | 99 %               | 99 %            | 99 %               |\n",
    "| **BERT Small**  | 99 %               | 99 %          | 99 %               | 99 %            | 99 %               |\n",
    "\n",
    "Les résultats de BERT Small et TinyBERT sont similaires, ce qui est courant lorsque le dataset est bien équilibré et de taille modérée. Cependant :\n",
    "- BERT Small a une capacité légèrement supérieure, ce qui peut le rendre plus performant sur des tâches ou datasets plus complexes.\n",
    "- TinyBERT est plus rapide et léger, ce qui est avantageux pour les systèmes avec des ressources limitées.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommandations\n",
    "1. **Choix du modèle** :\n",
    "   - Si les performances actuelles suffisent pour votre projet, **TinyBERT** est un excellent choix grâce à son efficacité.\n",
    "   - Si vous prévoyez de traiter des datasets plus complexes, **BERT Small** pourrait offrir un avantage.\n",
    "\n",
    "2. **Prochaines étapes** :\n",
    "   - **Déploiement** : Envisagez de déployer le modèle via une API ou une application web (par exemple, Flask, FastAPI).\n",
    "   - **Évaluation en conditions réelles** : Testez le modèle sur des données réelles pour vérifier sa robustesse.\n",
    "   - **Fine-tuning sur d'autres tâches** : Si vous souhaitez explorer d’autres tâches (comme la classification multi-label ou la détection d’entités nommées), BERT Small pourrait être un bon point de départ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_Lazy Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy Predict est une bibliothèque Python qui permet de tester rapidement plusieurs modèles de machine learning (ML) sur un dataset sans devoir coder chaque pipeline individuellement. Elle est utile pour :\n",
    "\n",
    "Obtenir une vue d'ensemble des performances de différents modèles ML.\n",
    "Comparer les modèles pour un problème spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.13-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (8.1.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lazypredict) (1.4.2)\n",
      "Collecting lightgbm (from lazypredict)\n",
      "  Downloading lightgbm-4.5.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting xgboost (from lazypredict)\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from click->lazypredict) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lightgbm->lazypredict) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from lightgbm->lazypredict) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from pandas->lazypredict) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from pandas->lazypredict) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from pandas->lazypredict) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from scikit-learn->lazypredict) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n",
      "Downloading lazypredict-0.2.13-py2.py3-none-any.whl (12 kB)\n",
      "Downloading lightgbm-4.5.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 73.9 MB/s eta 0:00:00\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ----------------- --------------------- 56.1/124.9 MB 275.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- --- 114.3/124.9 MB 270.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- 124.9/124.9 MB 221.8 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost, lightgbm, lazypredict\n",
      "Successfully installed lazypredict-0.2.13 lightgbm-4.5.0 xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install lazypredict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible : True\n",
      "Nom du GPU : NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Vérifier si CUDA est disponible\n",
    "print(\"GPU disponible :\", torch.cuda.is_available())\n",
    "print(\"Nom du GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU détecté\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appareil utilisé : cuda\n",
      "Colonnes dans le fichier d'entraînement : Index(['id', 'title', 'text', 'label', 'text_length'], dtype='object')\n",
      "Colonnes dans le fichier de test : Index(['id', 'title', 'text', 'label'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [01:23<42:53, 83.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Utilisation de Lazy Predict\u001b[39;00m\n\u001b[0;32m     40\u001b[0m clf \u001b[38;5;241m=\u001b[39m LazyClassifier(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, custom_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 41\u001b[0m models, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Afficher les résultats\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformances des modèles Lazy Predict :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\lazypredict\\Supervised.py:303\u001b[0m, in \u001b[0;36mLazyClassifier.fit\u001b[1;34m(self, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m    300\u001b[0m         steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, model())]\n\u001b[0;32m    301\u001b[0m     )\n\u001b[1;32m--> 303\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[name] \u001b[38;5;241m=\u001b[39m pipe\n\u001b[0;32m    305\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\pipeline.py:660\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    655\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    656\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    657\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    658\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    659\u001b[0m         )\n\u001b[1;32m--> 660\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:389\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    386\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    387\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:532\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, check_input, **fit_params)\u001b[0m\n\u001b[0;32m    529\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[1;32m--> 532\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    551\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[0;32m    552\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:189\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, seeds, total_n_estimators, verbose, check_input, fit_params)\u001b[0m\n\u001b[0;32m    187\u001b[0m     fit_params_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m curr_sample_weight\n\u001b[0;32m    188\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 189\u001b[0m     estimator_fit(X_, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# cannot use sample_weight, so use indexing\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     y_ \u001b[38;5;241m=\u001b[39m _safe_indexing(y, indices)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:1019\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1019\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chume\\.conda\\envs\\llama_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Vérifier si un GPU est disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Appareil utilisé :\", device)\n",
    "\n",
    "# Charger les fichiers\n",
    "train_file = \"C:/Users/chume/Projet_Final/data/processed/train_cleaned_final.csv\"\n",
    "test_file = \"C:/Users/chume/Projet_Final/data/processed/test_cleaned.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Vérifiez les colonnes\n",
    "print(\"Colonnes dans le fichier d'entraînement :\", train_data.columns)\n",
    "print(\"Colonnes dans le fichier de test :\", test_data.columns)\n",
    "\n",
    "# Assurez-vous d'utiliser les bonnes colonnes pour les textes et les labels\n",
    "X = train_data['text']\n",
    "y = train_data['label']\n",
    "\n",
    "# Vectorisation des données avec TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Limitez à 10 000 pour la mémoire GPU\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Réduction de dimension avec TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)  # Réduit à 300 dimensions\n",
    "X_reduced = svd.fit_transform(X_vec)\n",
    "\n",
    "# Division des données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Utilisation de Lazy Predict\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Performances des modèles Lazy Predict :\")\n",
    "print(models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
